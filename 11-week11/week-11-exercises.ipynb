{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1:  Wide and Deep Networks\n",
    "\n",
    "- Try changing your assumptions about which features should be considered by the \"non-linear\" (deep) part of the network.  Can you get any improvements?\n",
    "- Try using a different activation function in your wide and deep network. Do you get any improvements here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "# Reserve an item for prediction\n",
    "X_new = X_test[:3]\n",
    "import numpy as np\n",
    "# Note the offsets are used here to split up the features according to our input layer\n",
    "X_train_wide, X_train_deep = X_train[:, 2:4], np.delete(X_train, np.s_[2:4], axis=1)\n",
    "X_valid_wide, X_valid_deep = X_valid[:, 2:4], np.delete(X_valid, np.s_[2:4], axis=1)\n",
    "X_test_wide, X_test_deep = X_test[:, 2:4], np.delete(X_test, np.s_[2:4], axis=1)\n",
    "X_new_wide, X_new_deep = X_test_wide[:3], X_test_deep[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 4.1701 - dense_2_loss: 4.1430 - dense_3_loss: 4.4140 - dense_2_root_mean_squared_error: 2.0354 - dense_3_root_mean_squared_error: 2.1009 - val_loss: 2.7987 - val_dense_2_loss: 2.7598 - val_dense_3_loss: 3.1489 - val_dense_2_root_mean_squared_error: 1.6613 - val_dense_3_root_mean_squared_error: 1.7745\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 2.2248 - dense_2_loss: 2.1905 - dense_3_loss: 2.5334 - dense_2_root_mean_squared_error: 1.4800 - dense_3_root_mean_squared_error: 1.5917 - val_loss: 1.6084 - val_dense_2_loss: 1.5833 - val_dense_3_loss: 1.8346 - val_dense_2_root_mean_squared_error: 1.2583 - val_dense_3_root_mean_squared_error: 1.3545\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.4449 - dense_2_loss: 1.4273 - dense_3_loss: 1.6032 - dense_2_root_mean_squared_error: 1.1947 - dense_3_root_mean_squared_error: 1.2662 - val_loss: 1.2130 - val_dense_2_loss: 1.2027 - val_dense_3_loss: 1.3054 - val_dense_2_root_mean_squared_error: 1.0967 - val_dense_3_root_mean_squared_error: 1.1426\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.1105 - dense_2_loss: 1.1032 - dense_3_loss: 1.1769 - dense_2_root_mean_squared_error: 1.0503 - dense_3_root_mean_squared_error: 1.0848 - val_loss: 0.9272 - val_dense_2_loss: 0.9223 - val_dense_3_loss: 0.9716 - val_dense_2_root_mean_squared_error: 0.9604 - val_dense_3_root_mean_squared_error: 0.9857\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.8349 - dense_2_loss: 0.8313 - dense_3_loss: 0.8673 - dense_2_root_mean_squared_error: 0.9118 - dense_3_root_mean_squared_error: 0.9313 - val_loss: 0.7106 - val_dense_2_loss: 0.7078 - val_dense_3_loss: 0.7360 - val_dense_2_root_mean_squared_error: 0.8413 - val_dense_3_root_mean_squared_error: 0.8579\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6685 - dense_2_loss: 0.6667 - dense_3_loss: 0.6850 - dense_2_root_mean_squared_error: 0.8165 - dense_3_root_mean_squared_error: 0.8276 - val_loss: 0.5979 - val_dense_2_loss: 0.5966 - val_dense_3_loss: 0.6090 - val_dense_2_root_mean_squared_error: 0.7724 - val_dense_3_root_mean_squared_error: 0.7804\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5762 - dense_2_loss: 0.5754 - dense_3_loss: 0.5831 - dense_2_root_mean_squared_error: 0.7586 - dense_3_root_mean_squared_error: 0.7636 - val_loss: 0.5242 - val_dense_2_loss: 0.5237 - val_dense_3_loss: 0.5293 - val_dense_2_root_mean_squared_error: 0.7237 - val_dense_3_root_mean_squared_error: 0.7275\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5170 - dense_2_loss: 0.5166 - dense_3_loss: 0.5208 - dense_2_root_mean_squared_error: 0.7187 - dense_3_root_mean_squared_error: 0.7217 - val_loss: 0.4751 - val_dense_2_loss: 0.4747 - val_dense_3_loss: 0.4782 - val_dense_2_root_mean_squared_error: 0.6890 - val_dense_3_root_mean_squared_error: 0.6915\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 974us/step - loss: 0.4783 - dense_2_loss: 0.4779 - dense_3_loss: 0.4817 - dense_2_root_mean_squared_error: 0.6913 - dense_3_root_mean_squared_error: 0.6940 - val_loss: 0.4402 - val_dense_2_loss: 0.4400 - val_dense_3_loss: 0.4419 - val_dense_2_root_mean_squared_error: 0.6633 - val_dense_3_root_mean_squared_error: 0.6648\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 980us/step - loss: 0.4486 - dense_2_loss: 0.4483 - dense_3_loss: 0.4516 - dense_2_root_mean_squared_error: 0.6696 - dense_3_root_mean_squared_error: 0.6720 - val_loss: 0.4136 - val_dense_2_loss: 0.4135 - val_dense_3_loss: 0.4147 - val_dense_2_root_mean_squared_error: 0.6430 - val_dense_3_root_mean_squared_error: 0.6440\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 972us/step - loss: 0.4254 - dense_2_loss: 0.4251 - dense_3_loss: 0.4282 - dense_2_root_mean_squared_error: 0.6520 - dense_3_root_mean_squared_error: 0.6544 - val_loss: 0.3932 - val_dense_2_loss: 0.3930 - val_dense_3_loss: 0.3942 - val_dense_2_root_mean_squared_error: 0.6269 - val_dense_3_root_mean_squared_error: 0.6279\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 970us/step - loss: 0.4080 - dense_2_loss: 0.4077 - dense_3_loss: 0.4107 - dense_2_root_mean_squared_error: 0.6385 - dense_3_root_mean_squared_error: 0.6409 - val_loss: 0.3787 - val_dense_2_loss: 0.3785 - val_dense_3_loss: 0.3806 - val_dense_2_root_mean_squared_error: 0.6152 - val_dense_3_root_mean_squared_error: 0.6169\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 969us/step - loss: 0.3948 - dense_2_loss: 0.3944 - dense_3_loss: 0.3985 - dense_2_root_mean_squared_error: 0.6280 - dense_3_root_mean_squared_error: 0.6313 - val_loss: 0.3684 - val_dense_2_loss: 0.3682 - val_dense_3_loss: 0.3703 - val_dense_2_root_mean_squared_error: 0.6068 - val_dense_3_root_mean_squared_error: 0.6085\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 966us/step - loss: 0.3845 - dense_2_loss: 0.3841 - dense_3_loss: 0.3881 - dense_2_root_mean_squared_error: 0.6198 - dense_3_root_mean_squared_error: 0.6230 - val_loss: 0.3592 - val_dense_2_loss: 0.3590 - val_dense_3_loss: 0.3610 - val_dense_2_root_mean_squared_error: 0.5992 - val_dense_3_root_mean_squared_error: 0.6009\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3763 - dense_2_loss: 0.3760 - dense_3_loss: 0.3797 - dense_2_root_mean_squared_error: 0.6132 - dense_3_root_mean_squared_error: 0.6162 - val_loss: 0.3525 - val_dense_2_loss: 0.3523 - val_dense_3_loss: 0.3550 - val_dense_2_root_mean_squared_error: 0.5935 - val_dense_3_root_mean_squared_error: 0.5958\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3701 - dense_2_loss: 0.3696 - dense_3_loss: 0.3745 - dense_2_root_mean_squared_error: 0.6080 - dense_3_root_mean_squared_error: 0.6120 - val_loss: 0.3456 - val_dense_2_loss: 0.3455 - val_dense_3_loss: 0.3471 - val_dense_2_root_mean_squared_error: 0.5878 - val_dense_3_root_mean_squared_error: 0.5891\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3649 - dense_2_loss: 0.3644 - dense_3_loss: 0.3690 - dense_2_root_mean_squared_error: 0.6037 - dense_3_root_mean_squared_error: 0.6075 - val_loss: 0.3417 - val_dense_2_loss: 0.3415 - val_dense_3_loss: 0.3435 - val_dense_2_root_mean_squared_error: 0.5844 - val_dense_3_root_mean_squared_error: 0.5861\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3610 - dense_2_loss: 0.3606 - dense_3_loss: 0.3651 - dense_2_root_mean_squared_error: 0.6005 - dense_3_root_mean_squared_error: 0.6043 - val_loss: 0.3380 - val_dense_2_loss: 0.3378 - val_dense_3_loss: 0.3404 - val_dense_2_root_mean_squared_error: 0.5812 - val_dense_3_root_mean_squared_error: 0.5834\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3569 - dense_2_loss: 0.3564 - dense_3_loss: 0.3614 - dense_2_root_mean_squared_error: 0.5970 - dense_3_root_mean_squared_error: 0.6012 - val_loss: 0.3350 - val_dense_2_loss: 0.3347 - val_dense_3_loss: 0.3373 - val_dense_2_root_mean_squared_error: 0.5786 - val_dense_3_root_mean_squared_error: 0.5808\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3546 - dense_2_loss: 0.3541 - dense_3_loss: 0.3593 - dense_2_root_mean_squared_error: 0.5951 - dense_3_root_mean_squared_error: 0.5994 - val_loss: 0.3339 - val_dense_2_loss: 0.3336 - val_dense_3_loss: 0.3365 - val_dense_2_root_mean_squared_error: 0.5776 - val_dense_3_root_mean_squared_error: 0.5801\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "input_wide = tf.keras.layers.Input(shape=[2])  # features 0 to 4\n",
    "input_deep = tf.keras.layers.Input(shape=[6])  # features 2 to 7\n",
    "norm_layer_wide = tf.keras.layers.Normalization()\n",
    "norm_layer_deep = tf.keras.layers.Normalization()\n",
    "norm_wide = norm_layer_wide(input_wide)\n",
    "norm_deep = norm_layer_deep(input_deep)\n",
    "hidden1 = tf.keras.layers.Dense(30,activation='softmax')(norm_deep)\n",
    "hidden2 = tf.keras.layers.Dense(30, activation=\"softmax\")(hidden1)\n",
    "concat = tf.keras.layers.concatenate([norm_wide, hidden2])\n",
    "output = tf.keras.layers.Dense(1)(concat)\n",
    "aux_output = tf.keras.layers.Dense(1)(hidden2)\n",
    "model = tf.keras.Model(inputs=[input_wide, input_deep],\n",
    "                       outputs=[output, aux_output])\n",
    "\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=1e-3)\n",
    "model.compile(loss=(\"mse\", \"mse\"), loss_weights=(0.9, 0.1), optimizer=optimizer,\n",
    "              metrics=[\"RootMeanSquaredError\"])\n",
    "\n",
    "# Remember to adapt the normalization layers!!!\n",
    "norm_layer_wide.adapt(X_train_wide)\n",
    "norm_layer_deep.adapt(X_train_deep)\n",
    "history = model.fit(\n",
    "    (X_train_wide, X_train_deep), (y_train, y_train), epochs=20,\n",
    "    validation_data=((X_valid_wide, X_valid_deep), (y_valid, y_valid))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 1ms/step - loss: 0.3498 - dense_2_loss: 0.3495 - dense_3_loss: 0.3527 - dense_2_root_mean_squared_error: 0.5911 - dense_3_root_mean_squared_error: 0.5939\n",
      "\n",
      "Weighed Sum of Losses: 0.3497806787490845\n",
      "Main Loss: 0.3494572341442108\n",
      "Aux Loss: 0.3526919484138489\n",
      "Main RMSE: 0.591149091720581\n",
      "Aux RMSE: 0.5938787460327148\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "eval_results = model.evaluate((X_test_wide, X_test_deep), (y_test, y_test))\n",
    "weighted_sum_of_losses, main_loss, aux_loss, main_rmse, aux_rmse = eval_results\n",
    "print(f\"\"\"\n",
    "Weighed Sum of Losses: {weighted_sum_of_losses}\n",
    "Main Loss: {main_loss}\n",
    "Aux Loss: {aux_loss}\n",
    "Main RMSE: {main_rmse}\n",
    "Aux RMSE: {aux_rmse}\n",
    "      \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Modifying the Wide and Deep Network\n",
    "\n",
    "1. Modify the constructor of the wide and deep network to take parameters that govern the number of hidden layers and their width.  Now try running the new network with different properties for your hidden layers.  Can you improve your performance?\n",
    "\n",
    "2. Modify the network class to use BatchNormalization, but add normalization after the activation function.  Examine your performance.\n",
    "\n",
    "3. Modify the preceding class to use BatchNormalization before the activation function.  Examine your performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.7640 - dense_2_loss: 1.6491 - dense_3_loss: 2.7978 - dense_2_root_mean_squared_error: 1.2842 - dense_3_root_mean_squared_error: 1.6727 - val_loss: 1.2352 - val_dense_2_loss: 1.2252 - val_dense_3_loss: 1.3246 - val_dense_2_root_mean_squared_error: 1.1069 - val_dense_3_root_mean_squared_error: 1.1509\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5183 - dense_2_loss: 0.5218 - dense_3_loss: 0.4866 - dense_2_root_mean_squared_error: 0.7224 - dense_3_root_mean_squared_error: 0.6976 - val_loss: 0.4963 - val_dense_2_loss: 0.4982 - val_dense_3_loss: 0.4793 - val_dense_2_root_mean_squared_error: 0.7058 - val_dense_3_root_mean_squared_error: 0.6923\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4611 - dense_2_loss: 0.4635 - dense_3_loss: 0.4397 - dense_2_root_mean_squared_error: 0.6808 - dense_3_root_mean_squared_error: 0.6631 - val_loss: 0.4072 - val_dense_2_loss: 0.4081 - val_dense_3_loss: 0.3995 - val_dense_2_root_mean_squared_error: 0.6388 - val_dense_3_root_mean_squared_error: 0.6321\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4359 - dense_2_loss: 0.4364 - dense_3_loss: 0.4312 - dense_2_root_mean_squared_error: 0.6606 - dense_3_root_mean_squared_error: 0.6566 - val_loss: 0.3797 - val_dense_2_loss: 0.3794 - val_dense_3_loss: 0.3827 - val_dense_2_root_mean_squared_error: 0.6160 - val_dense_3_root_mean_squared_error: 0.6186\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4202 - dense_2_loss: 0.4202 - dense_3_loss: 0.4203 - dense_2_root_mean_squared_error: 0.6482 - dense_3_root_mean_squared_error: 0.6483 - val_loss: 0.3800 - val_dense_2_loss: 0.3793 - val_dense_3_loss: 0.3861 - val_dense_2_root_mean_squared_error: 0.6159 - val_dense_3_root_mean_squared_error: 0.6214\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4103 - dense_2_loss: 0.4099 - dense_3_loss: 0.4135 - dense_2_root_mean_squared_error: 0.6403 - dense_3_root_mean_squared_error: 0.6430 - val_loss: 0.3826 - val_dense_2_loss: 0.3824 - val_dense_3_loss: 0.3844 - val_dense_2_root_mean_squared_error: 0.6184 - val_dense_3_root_mean_squared_error: 0.6200\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4008 - dense_2_loss: 0.4005 - dense_3_loss: 0.4034 - dense_2_root_mean_squared_error: 0.6328 - dense_3_root_mean_squared_error: 0.6351 - val_loss: 0.3803 - val_dense_2_loss: 0.3796 - val_dense_3_loss: 0.3863 - val_dense_2_root_mean_squared_error: 0.6161 - val_dense_3_root_mean_squared_error: 0.6215\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4005 - dense_2_loss: 0.4000 - dense_3_loss: 0.4044 - dense_2_root_mean_squared_error: 0.6325 - dense_3_root_mean_squared_error: 0.6359 - val_loss: 0.3709 - val_dense_2_loss: 0.3708 - val_dense_3_loss: 0.3717 - val_dense_2_root_mean_squared_error: 0.6089 - val_dense_3_root_mean_squared_error: 0.6097\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3987 - dense_2_loss: 0.3982 - dense_3_loss: 0.4025 - dense_2_root_mean_squared_error: 0.6311 - dense_3_root_mean_squared_error: 0.6344 - val_loss: 0.3722 - val_dense_2_loss: 0.3718 - val_dense_3_loss: 0.3765 - val_dense_2_root_mean_squared_error: 0.6097 - val_dense_3_root_mean_squared_error: 0.6136\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3986 - dense_2_loss: 0.3982 - dense_3_loss: 0.4021 - dense_2_root_mean_squared_error: 0.6310 - dense_3_root_mean_squared_error: 0.6341 - val_loss: 0.3725 - val_dense_2_loss: 0.3721 - val_dense_3_loss: 0.3756 - val_dense_2_root_mean_squared_error: 0.6100 - val_dense_3_root_mean_squared_error: 0.6129\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3937 - dense_2_loss: 0.3932 - dense_3_loss: 0.3984 - dense_2_root_mean_squared_error: 0.6270 - dense_3_root_mean_squared_error: 0.6312 - val_loss: 0.3436 - val_dense_2_loss: 0.3432 - val_dense_3_loss: 0.3474 - val_dense_2_root_mean_squared_error: 0.5859 - val_dense_3_root_mean_squared_error: 0.5894\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3875 - dense_2_loss: 0.3870 - dense_3_loss: 0.3915 - dense_2_root_mean_squared_error: 0.6221 - dense_3_root_mean_squared_error: 0.6257 - val_loss: 0.3626 - val_dense_2_loss: 0.3620 - val_dense_3_loss: 0.3680 - val_dense_2_root_mean_squared_error: 0.6016 - val_dense_3_root_mean_squared_error: 0.6066\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3888 - dense_2_loss: 0.3884 - dense_3_loss: 0.3922 - dense_2_root_mean_squared_error: 0.6232 - dense_3_root_mean_squared_error: 0.6263 - val_loss: 0.3489 - val_dense_2_loss: 0.3484 - val_dense_3_loss: 0.3528 - val_dense_2_root_mean_squared_error: 0.5903 - val_dense_3_root_mean_squared_error: 0.5939\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3770 - dense_2_loss: 0.3764 - dense_3_loss: 0.3823 - dense_2_root_mean_squared_error: 0.6135 - dense_3_root_mean_squared_error: 0.6183 - val_loss: 0.3398 - val_dense_2_loss: 0.3389 - val_dense_3_loss: 0.3478 - val_dense_2_root_mean_squared_error: 0.5822 - val_dense_3_root_mean_squared_error: 0.5897\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3782 - dense_2_loss: 0.3776 - dense_3_loss: 0.3834 - dense_2_root_mean_squared_error: 0.6145 - dense_3_root_mean_squared_error: 0.6192 - val_loss: 0.3477 - val_dense_2_loss: 0.3459 - val_dense_3_loss: 0.3642 - val_dense_2_root_mean_squared_error: 0.5881 - val_dense_3_root_mean_squared_error: 0.6035\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3797 - dense_2_loss: 0.3790 - dense_3_loss: 0.3861 - dense_2_root_mean_squared_error: 0.6156 - dense_3_root_mean_squared_error: 0.6213 - val_loss: 0.3295 - val_dense_2_loss: 0.3279 - val_dense_3_loss: 0.3438 - val_dense_2_root_mean_squared_error: 0.5726 - val_dense_3_root_mean_squared_error: 0.5863\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3829 - dense_2_loss: 0.3823 - dense_3_loss: 0.3885 - dense_2_root_mean_squared_error: 0.6183 - dense_3_root_mean_squared_error: 0.6233 - val_loss: 0.3522 - val_dense_2_loss: 0.3501 - val_dense_3_loss: 0.3708 - val_dense_2_root_mean_squared_error: 0.5917 - val_dense_3_root_mean_squared_error: 0.6089\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3703 - dense_2_loss: 0.3695 - dense_3_loss: 0.3774 - dense_2_root_mean_squared_error: 0.6079 - dense_3_root_mean_squared_error: 0.6144 - val_loss: 0.3585 - val_dense_2_loss: 0.3572 - val_dense_3_loss: 0.3699 - val_dense_2_root_mean_squared_error: 0.5977 - val_dense_3_root_mean_squared_error: 0.6082\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3680 - dense_2_loss: 0.3672 - dense_3_loss: 0.3753 - dense_2_root_mean_squared_error: 0.6060 - dense_3_root_mean_squared_error: 0.6126 - val_loss: 0.3323 - val_dense_2_loss: 0.3303 - val_dense_3_loss: 0.3505 - val_dense_2_root_mean_squared_error: 0.5747 - val_dense_3_root_mean_squared_error: 0.5920\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3675 - dense_2_loss: 0.3668 - dense_3_loss: 0.3746 - dense_2_root_mean_squared_error: 0.6056 - dense_3_root_mean_squared_error: 0.6120 - val_loss: 0.3441 - val_dense_2_loss: 0.3417 - val_dense_3_loss: 0.3661 - val_dense_2_root_mean_squared_error: 0.5845 - val_dense_3_root_mean_squared_error: 0.6051\n",
      "162/162 [==============================] - 0s 623us/step - loss: 0.3435 - dense_2_loss: 0.3425 - dense_3_loss: 0.3531 - dense_2_root_mean_squared_error: 0.5852 - dense_3_root_mean_squared_error: 0.5942\n",
      "\n",
      "Weighed Sum of Losses: 0.34352004528045654\n",
      "Main Loss: 0.34245535731315613\n",
      "Aux Loss: 0.35310137271881104\n",
      "Main RMSE: 0.585196852684021\n",
      "Aux RMSE: 0.5942233204841614\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "input_wide = tf.keras.layers.Input(shape=[2])  # features 0 to 4\n",
    "input_deep = tf.keras.layers.Input(shape=[6])  # features 2 to 7\n",
    "\n",
    "norm_layer_wide = tf.keras.layers.Normalization()\n",
    "norm_layer_deep = tf.keras.layers.Normalization()\n",
    "\n",
    "norm_wide = norm_layer_wide(input_wide)\n",
    "norm_deep = norm_layer_deep(input_deep)\n",
    "\n",
    "norm_deep_normalised=tf.keras.layers.BatchNormalization()(norm_deep)\n",
    "hidden1 = tf.keras.layers.Dense(30, activation='softmax')(norm_deep_normalised)\n",
    "hidden1_normalized = tf.keras.layers.BatchNormalization()(hidden1)\n",
    "\n",
    "hidden2 = tf.keras.layers.Dense(30, activation=\"softmax\")(hidden1_normalized)\n",
    "hidden2_normalized = tf.keras.layers.BatchNormalization()(hidden2)\n",
    "\n",
    "concat = tf.keras.layers.concatenate([norm_wide, hidden2_normalized])\n",
    "\n",
    "output = tf.keras.layers.Dense(1)(concat)\n",
    "aux_output = tf.keras.layers.Dense(1)(hidden2_normalized)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_wide, input_deep],\n",
    "                       outputs=[output, aux_output])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "model.compile(loss=(\"mse\", \"mse\"), loss_weights=(0.9, 0.1), optimizer=optimizer,\n",
    "              metrics=[\"RootMeanSquaredError\"])\n",
    "\n",
    "# Remember to adapt the normalization layers!!!\n",
    "norm_layer_wide.adapt(X_train_wide)\n",
    "norm_layer_deep.adapt(X_train_deep)\n",
    "\n",
    "history = model.fit(\n",
    "    (X_train_wide, X_train_deep), (y_train, y_train), epochs=20,\n",
    "    validation_data=((X_valid_wide, X_valid_deep), (y_valid, y_valid))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 974us/step - loss: 0.3435 - dense_2_loss: 0.3425 - dense_3_loss: 0.3531 - dense_2_root_mean_squared_error: 0.5852 - dense_3_root_mean_squared_error: 0.5942\n",
      "\n",
      "Weighed Sum of Losses: 0.34352004528045654\n",
      "Main Loss: 0.34245535731315613\n",
      "Aux Loss: 0.35310137271881104\n",
      "Main RMSE: 0.585196852684021\n",
      "Aux RMSE: 0.5942233204841614\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_results = model.evaluate((X_test_wide, X_test_deep), (y_test, y_test))\n",
    "weighted_sum_of_losses, main_loss, aux_loss, main_rmse, aux_rmse = eval_results\n",
    "\n",
    "print(f\"\"\"\n",
    "Weighed Sum of Losses: {weighted_sum_of_losses}\n",
    "Main Loss: {main_loss}\n",
    "Aux Loss: {aux_loss}\n",
    "Main RMSE: {main_rmse}\n",
    "Aux RMSE: {aux_rmse}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Comparing SELU and ReLU\n",
    "\n",
    "Let's examine the performance of SELU and ReLU in the context of a deep network with 100 layers.  I've set up the example below to use SELU and Lecun Normalization.  Examine the performance, and then contrast using ReLU and He normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=[28, 28]))\n",
    "\n",
    "# 100 hidden layers\n",
    "for layer in range(100):\n",
    "    # model.add(tf.keras.layers.Dense(100, activation=\"selu\",\n",
    "    #                                 kernel_initializer=\"lecun_normal\"))\n",
    "    model.add(tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                                    kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.legacy.SGD(learning_rate=0.001),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data and train the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 18s 9ms/step - loss: 2.0980 - accuracy: 0.1969 - val_loss: 1.8966 - val_accuracy: 0.2602\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 1.7103 - accuracy: 0.3469 - val_loss: 1.5307 - val_accuracy: 0.4194\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 1.3293 - accuracy: 0.4725 - val_loss: 1.1189 - val_accuracy: 0.5244\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 1.1417 - accuracy: 0.5395 - val_loss: 1.0240 - val_accuracy: 0.6002\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 1.0020 - accuracy: 0.6135 - val_loss: 1.1292 - val_accuracy: 0.5320\n"
     ]
    }
   ],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
    "X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\n",
    "X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]\n",
    "\n",
    "# Remember to scale the inputs!\n",
    "X_train, X_valid, X_test = X_train / 255, X_valid / 255, X_test / 255\n",
    "\n",
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "\n",
    "\n",
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, epochs=5,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 3.5996 - accuracy: 0.0896\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.599595785140991, 0.08959999680519104]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results = model.evaluate(X_test,y_test)\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.599595785140991, 0.08959999680519104]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results_relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.484071135520935, 0.5303999781608582]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results_selu\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

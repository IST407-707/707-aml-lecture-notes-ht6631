{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Use the following code to explore l1 and l2 regularization.  Try different combinations of l1, l2, and alpha.  What works the best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up some data\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
    "X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\n",
    "X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]\n",
    "X_train, X_valid, X_test = X_train / 255, X_valid / 255, X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 8.3345 - accuracy: 0.6332 - val_loss: 1.9222 - val_accuracy: 0.6596\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 1.6999 - accuracy: 0.6597 - val_loss: 1.5453 - val_accuracy: 0.6950\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 1.5351 - accuracy: 0.6778 - val_loss: 1.5106 - val_accuracy: 0.6778\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 1.4928 - accuracy: 0.6815 - val_loss: 1.4602 - val_accuracy: 0.7138\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 1.4651 - accuracy: 0.6867 - val_loss: 1.4840 - val_accuracy: 0.6888\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 1.4405 - accuracy: 0.6927 - val_loss: 1.4581 - val_accuracy: 0.6844\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 1.4283 - accuracy: 0.6943 - val_loss: 1.3844 - val_accuracy: 0.7018\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 1.4159 - accuracy: 0.7034 - val_loss: 1.3561 - val_accuracy: 0.7206\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 1.4050 - accuracy: 0.7122 - val_loss: 1.3770 - val_accuracy: 0.7298\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 1.3985 - accuracy: 0.7197 - val_loss: 1.3460 - val_accuracy: 0.7400\n",
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 17.3342 - accuracy: 0.1065 - val_loss: 11.2547 - val_accuracy: 0.0980\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 11.2428 - accuracy: 0.0972 - val_loss: 11.2290 - val_accuracy: 0.1054\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 11.2427 - accuracy: 0.0989 - val_loss: 11.2558 - val_accuracy: 0.1054\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 11.2428 - accuracy: 0.0987 - val_loss: 11.2292 - val_accuracy: 0.1016\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 11.2428 - accuracy: 0.0983 - val_loss: 11.2567 - val_accuracy: 0.1006\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 11.2428 - accuracy: 0.0978 - val_loss: 11.2295 - val_accuracy: 0.0900\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 11.2427 - accuracy: 0.1013 - val_loss: 11.2574 - val_accuracy: 0.0980\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 11.2428 - accuracy: 0.0985 - val_loss: 11.2295 - val_accuracy: 0.0980\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 11.2428 - accuracy: 0.0985 - val_loss: 11.2578 - val_accuracy: 0.1016\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 11.2428 - accuracy: 0.0998 - val_loss: 11.2280 - val_accuracy: 0.0934\n",
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 231.8731 - accuracy: 0.0996 - val_loss: 225.2433 - val_accuracy: 0.0980\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 225.8016 - accuracy: 0.0975 - val_loss: 226.3398 - val_accuracy: 0.1054\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 225.8016 - accuracy: 0.0990 - val_loss: 225.3508 - val_accuracy: 0.1054\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 225.8024 - accuracy: 0.0991 - val_loss: 226.2227 - val_accuracy: 0.1016\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 225.8010 - accuracy: 0.0991 - val_loss: 225.4474 - val_accuracy: 0.1006\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 225.8010 - accuracy: 0.0975 - val_loss: 226.1679 - val_accuracy: 0.0900\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 225.8024 - accuracy: 0.1012 - val_loss: 225.4849 - val_accuracy: 0.0980\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 225.8034 - accuracy: 0.0989 - val_loss: 226.2145 - val_accuracy: 0.0980\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 225.8018 - accuracy: 0.0980 - val_loss: 225.3421 - val_accuracy: 0.1016\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 225.8040 - accuracy: 0.0997 - val_loss: 226.2256 - val_accuracy: 0.0934\n",
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 3.1313 - accuracy: 0.7691 - val_loss: 1.8613 - val_accuracy: 0.8246\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 1.4272 - accuracy: 0.8132 - val_loss: 1.1269 - val_accuracy: 0.8218\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 1.0213 - accuracy: 0.8163 - val_loss: 0.9326 - val_accuracy: 0.8200\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.9177 - accuracy: 0.8162 - val_loss: 0.8775 - val_accuracy: 0.8256\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.8862 - accuracy: 0.8172 - val_loss: 0.8678 - val_accuracy: 0.8192\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.8747 - accuracy: 0.8183 - val_loss: 0.8786 - val_accuracy: 0.8098\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.8709 - accuracy: 0.8170 - val_loss: 0.8524 - val_accuracy: 0.8216\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.8673 - accuracy: 0.8170 - val_loss: 0.8531 - val_accuracy: 0.8162\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.8644 - accuracy: 0.8205 - val_loss: 0.8423 - val_accuracy: 0.8290\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.8610 - accuracy: 0.8197 - val_loss: 0.8437 - val_accuracy: 0.8270\n",
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 5.1194 - accuracy: 0.5708 - val_loss: 2.0228 - val_accuracy: 0.5704\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 2.0122 - accuracy: 0.5432 - val_loss: 1.9957 - val_accuracy: 0.5446\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 1.9948 - accuracy: 0.5376 - val_loss: 1.9815 - val_accuracy: 0.5528\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 1.9876 - accuracy: 0.5402 - val_loss: 1.9755 - val_accuracy: 0.5538\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 1.9822 - accuracy: 0.5327 - val_loss: 1.9806 - val_accuracy: 0.5442\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 1.9787 - accuracy: 0.5283 - val_loss: 1.9893 - val_accuracy: 0.4712\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 1.9756 - accuracy: 0.5220 - val_loss: 1.9736 - val_accuracy: 0.4976\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 1.9731 - accuracy: 0.5217 - val_loss: 1.9665 - val_accuracy: 0.5054\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 1.9707 - accuracy: 0.5133 - val_loss: 1.9663 - val_accuracy: 0.5142\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 1.9679 - accuracy: 0.5127 - val_loss: 1.9618 - val_accuracy: 0.5460\n",
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 5.4205 - accuracy: 0.1146 - val_loss: 2.3026 - val_accuracy: 0.0980\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 2.3028 - accuracy: 0.0971 - val_loss: 2.3026 - val_accuracy: 0.1054\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 2.3027 - accuracy: 0.0992 - val_loss: 2.3026 - val_accuracy: 0.1054\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 2.3028 - accuracy: 0.0987 - val_loss: 2.3025 - val_accuracy: 0.1016\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 2.3028 - accuracy: 0.0981 - val_loss: 2.3030 - val_accuracy: 0.1006\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 2.3028 - accuracy: 0.0976 - val_loss: 2.3031 - val_accuracy: 0.0900\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 2.3027 - accuracy: 0.1013 - val_loss: 2.3024 - val_accuracy: 0.0980\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 2.3028 - accuracy: 0.0988 - val_loss: 2.3031 - val_accuracy: 0.0980\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 2.3028 - accuracy: 0.0986 - val_loss: 2.3028 - val_accuracy: 0.1016\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 2.3028 - accuracy: 0.0997 - val_loss: 2.3030 - val_accuracy: 0.0934\n"
     ]
    }
   ],
   "source": [
    "# You'll need to modify this to try different regularizers.  To make your life easier,\n",
    "# you might want to create a function to generate your model\n",
    "\n",
    "tf.random.set_seed(42) \n",
    "\n",
    "from functools import partial\n",
    "def bdfunction(norm,alpha):\n",
    "    if norm==1:\n",
    "        return tf.keras.regularizers.l1(alpha)\n",
    "    else:\n",
    "        return tf.keras.regularizers.l2(alpha)\n",
    "\n",
    "acc_list=[]\n",
    "\n",
    "for norm in [1,2]:\n",
    "    for alpha in [0.01,0.1,0.5]:\n",
    "        RegularizedDense = partial(tf.keras.layers.Dense,\n",
    "                                activation=\"relu\",\n",
    "                                kernel_initializer=\"he_normal\",\n",
    "                                kernel_regularizer=bdfunction(norm,alpha))\n",
    "\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "            RegularizedDense(100),\n",
    "            RegularizedDense(100),\n",
    "            RegularizedDense(10, activation=\"softmax\")\n",
    "        ])\n",
    "        optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=0.02)\n",
    "        model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "        history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "        test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "        acc_list.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7300999760627747,\n",
       " 0.10000000149011612,\n",
       " 0.10000000149011612,\n",
       " 0.8134999871253967,\n",
       " 0.5468000173568726,\n",
       " 0.10000000149011612]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "1. Build two networks for processing the MNIST Fashion data.  Both networks should have three dense layers (but may also have dropout layers or activation function layers) with 100 neurons.\n",
    "\n",
    "    1.  A self-normalizing neural network with dropout.\n",
    "    2.  A network without normalization but using dropout.\n",
    "    3.  A network with BatchNormalization.\n",
    "\n",
    "Try each with either a 1cycle or performance schedule.  Which works the best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
    "from tensorflow.keras.utils import normalize\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Clear previous TensorFlow session\n",
    "K.clear_session()\n",
    "\n",
    "# Load the dataset\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# Reshape the data\n",
    "X_train = X_train.reshape((-1, 28*28))\n",
    "X_test = X_test.reshape((-1, 28*28))\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Split the training set into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_normalizing_nn_with_dropout():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(100, activation='selu', kernel_initializer='lecun_normal', input_shape=(28*28,)))\n",
    "    model.add(layers.AlphaDropout(0.25))\n",
    "    model.add(layers.Dense(100, activation='selu', kernel_initializer='lecun_normal'))\n",
    "    model.add(layers.AlphaDropout(0.25))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_with_dropout():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(100, activation='relu', input_shape=(28*28,)))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "    model.add(layers.Dense(100, activation='relu'))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_with_batch_normalization():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(100, activation='relu', input_shape=(28*28,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(100, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule_1cycle(epoch, lr):\n",
    "    max_lr = 0.1\n",
    "    end_percentage = 0.1\n",
    "    total_steps = 5\n",
    "    anneal_steps = int(total_steps * end_percentage)\n",
    "    steps = epoch % total_steps\n",
    "    if steps == 0 and epoch != 0:\n",
    "        return lr * 10\n",
    "    if steps < anneal_steps:\n",
    "        return (max_lr - lr) / anneal_steps * steps + lr\n",
    "    else:\n",
    "        return (lr - max_lr) / (total_steps - anneal_steps) * (steps - anneal_steps) + max_lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule_performance(epoch, lr):\n",
    "    if epoch < 5:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "750/750 [==============================] - 3s 3ms/step - loss: 0.8132 - accuracy: 0.6945 - val_loss: 0.5912 - val_accuracy: 0.8069 - lr: 0.1000\n",
      "Epoch 2/10\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.5819 - accuracy: 0.7878 - val_loss: 0.6114 - val_accuracy: 0.8004 - lr: 0.1000\n",
      "Epoch 3/10\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.5346 - accuracy: 0.8054 - val_loss: 0.5248 - val_accuracy: 0.8308 - lr: 0.1000\n",
      "Epoch 4/10\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.5077 - accuracy: 0.8164 - val_loss: 0.5010 - val_accuracy: 0.8358 - lr: 0.1000\n",
      "Epoch 5/10\n",
      "750/750 [==============================] - 3s 4ms/step - loss: 0.4852 - accuracy: 0.8242 - val_loss: 0.5192 - val_accuracy: 0.8336 - lr: 0.1000\n",
      "Epoch 6/10\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 228881968.0000 - accuracy: 0.1030 - val_loss: 4844734.5000 - val_accuracy: 0.0997 - lr: 1.0000\n",
      "Epoch 7/10\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 3136974.5000 - accuracy: 0.1001 - val_loss: 4806542.0000 - val_accuracy: 0.0997 - lr: 0.2800\n",
      "Epoch 8/10\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 3116864.5000 - accuracy: 0.1001 - val_loss: 4783056.5000 - val_accuracy: 0.0997 - lr: 0.1720\n",
      "Epoch 9/10\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 3102826.7500 - accuracy: 0.1001 - val_loss: 4763497.0000 - val_accuracy: 0.0997 - lr: 0.1432\n",
      "Epoch 10/10\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 3090475.0000 - accuracy: 0.1001 - val_loss: 4745152.5000 - val_accuracy: 0.0997 - lr: 0.1346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 1/10\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 1.4740 - accuracy: 0.4595 - val_loss: 0.7803 - val_accuracy: 0.7061 - lr: 0.0100\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 2/10\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.9517 - accuracy: 0.6344 - val_loss: 0.7227 - val_accuracy: 0.7352 - lr: 0.0100\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 3/10\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.8200 - accuracy: 0.6877 - val_loss: 0.6966 - val_accuracy: 0.7535 - lr: 0.0100\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 4/10\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.7536 - accuracy: 0.7164 - val_loss: 0.6764 - val_accuracy: 0.7645 - lr: 0.0100\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 5/10\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.7097 - accuracy: 0.7336 - val_loss: 0.6663 - val_accuracy: 0.7718 - lr: 0.0100\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.009048374369740486.\n",
      "Epoch 6/10\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.6798 - accuracy: 0.7462 - val_loss: 0.6371 - val_accuracy: 0.7850 - lr: 0.0090\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.008187307976186275.\n",
      "Epoch 7/10\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.6585 - accuracy: 0.7547 - val_loss: 0.6244 - val_accuracy: 0.7911 - lr: 0.0082\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.007408182602375746.\n",
      "Epoch 8/10\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.6416 - accuracy: 0.7634 - val_loss: 0.6038 - val_accuracy: 0.7989 - lr: 0.0074\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.006703200750052929.\n",
      "Epoch 9/10\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.6284 - accuracy: 0.7674 - val_loss: 0.6119 - val_accuracy: 0.7956 - lr: 0.0067\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.006065306719392538.\n",
      "Epoch 10/10\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.6215 - accuracy: 0.7717 - val_loss: 0.6055 - val_accuracy: 0.7979 - lr: 0.0061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.7226 - accuracy: 0.7420 - val_loss: 0.5017 - val_accuracy: 0.8146 - lr: 0.1000\n",
      "Epoch 2/10\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.5087 - accuracy: 0.8186 - val_loss: 0.4715 - val_accuracy: 0.8280 - lr: 0.1000\n",
      "Epoch 3/10\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.4598 - accuracy: 0.8337 - val_loss: 0.3901 - val_accuracy: 0.8538 - lr: 0.1000\n",
      "Epoch 4/10\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.4315 - accuracy: 0.8451 - val_loss: 0.3943 - val_accuracy: 0.8553 - lr: 0.1000\n",
      "Epoch 5/10\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.4095 - accuracy: 0.8514 - val_loss: 0.3774 - val_accuracy: 0.8577 - lr: 0.1000\n",
      "Epoch 6/10\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 504640672.0000 - accuracy: 0.1015 - val_loss: 2.3054 - val_accuracy: 0.0966 - lr: 1.0000\n",
      "Epoch 7/10\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 2.3037 - accuracy: 0.0985 - val_loss: 2.3033 - val_accuracy: 0.1016 - lr: 0.2800\n",
      "Epoch 8/10\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 2.3033 - accuracy: 0.1008 - val_loss: 2.3038 - val_accuracy: 0.0987 - lr: 0.1720\n",
      "Epoch 9/10\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 2.3033 - accuracy: 0.0990 - val_loss: 2.3028 - val_accuracy: 0.1009 - lr: 0.1432\n",
      "Epoch 10/10\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 2.3031 - accuracy: 0.0994 - val_loss: 2.3033 - val_accuracy: 0.1009 - lr: 0.1346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 1/10\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 1.2010 - accuracy: 0.5864 - val_loss: 0.7330 - val_accuracy: 0.7531 - lr: 0.0100\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 2/10\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.7820 - accuracy: 0.7268 - val_loss: 0.6184 - val_accuracy: 0.7889 - lr: 0.0100\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 3/10\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.6820 - accuracy: 0.7623 - val_loss: 0.5602 - val_accuracy: 0.8087 - lr: 0.0100\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 4/10\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.6210 - accuracy: 0.7842 - val_loss: 0.5241 - val_accuracy: 0.8184 - lr: 0.0100\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 5/10\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.5830 - accuracy: 0.7976 - val_loss: 0.4971 - val_accuracy: 0.8250 - lr: 0.0100\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.009048374369740486.\n",
      "Epoch 6/10\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.5533 - accuracy: 0.8075 - val_loss: 0.4799 - val_accuracy: 0.8288 - lr: 0.0090\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.008187307976186275.\n",
      "Epoch 7/10\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.5344 - accuracy: 0.8139 - val_loss: 0.4643 - val_accuracy: 0.8347 - lr: 0.0082\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.007408182602375746.\n",
      "Epoch 8/10\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.5203 - accuracy: 0.8192 - val_loss: 0.4535 - val_accuracy: 0.8361 - lr: 0.0074\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.006703200750052929.\n",
      "Epoch 9/10\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.5104 - accuracy: 0.8226 - val_loss: 0.4449 - val_accuracy: 0.8378 - lr: 0.0067\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.006065306719392538.\n",
      "Epoch 10/10\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.4985 - accuracy: 0.8273 - val_loss: 0.4378 - val_accuracy: 0.8425 - lr: 0.0061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.4922 - accuracy: 0.8209 - val_loss: 0.5047 - val_accuracy: 0.8106 - lr: 0.1000\n",
      "Epoch 2/10\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3822 - accuracy: 0.8616 - val_loss: 0.4275 - val_accuracy: 0.8433 - lr: 0.1000\n",
      "Epoch 3/10\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3537 - accuracy: 0.8704 - val_loss: 0.3725 - val_accuracy: 0.8653 - lr: 0.1000\n",
      "Epoch 4/10\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3328 - accuracy: 0.8766 - val_loss: 0.3989 - val_accuracy: 0.8541 - lr: 0.1000\n",
      "Epoch 5/10\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3085 - accuracy: 0.8856 - val_loss: 0.4102 - val_accuracy: 0.8566 - lr: 0.1000\n",
      "Epoch 6/10\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.5001 - accuracy: 0.8209 - val_loss: 0.4612 - val_accuracy: 0.8323 - lr: 1.0000\n",
      "Epoch 7/10\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3396 - accuracy: 0.8745 - val_loss: 0.3506 - val_accuracy: 0.8707 - lr: 0.2800\n",
      "Epoch 8/10\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3050 - accuracy: 0.8873 - val_loss: 0.3389 - val_accuracy: 0.8792 - lr: 0.1720\n",
      "Epoch 9/10\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2908 - accuracy: 0.8929 - val_loss: 0.3406 - val_accuracy: 0.8784 - lr: 0.1432\n",
      "Epoch 10/10\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2839 - accuracy: 0.8944 - val_loss: 0.3255 - val_accuracy: 0.8818 - lr: 0.1346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 1/10\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.6127 - accuracy: 0.7920 - val_loss: 0.4765 - val_accuracy: 0.8341 - lr: 0.0100\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 2/10\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.4346 - accuracy: 0.8461 - val_loss: 0.4056 - val_accuracy: 0.8575 - lr: 0.0100\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 3/10\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3931 - accuracy: 0.8607 - val_loss: 0.3878 - val_accuracy: 0.8592 - lr: 0.0100\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 4/10\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3685 - accuracy: 0.8677 - val_loss: 0.3866 - val_accuracy: 0.8609 - lr: 0.0100\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 5/10\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3496 - accuracy: 0.8748 - val_loss: 0.3652 - val_accuracy: 0.8660 - lr: 0.0100\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.009048374369740486.\n",
      "Epoch 6/10\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3326 - accuracy: 0.8802 - val_loss: 0.3536 - val_accuracy: 0.8744 - lr: 0.0090\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.008187307976186275.\n",
      "Epoch 7/10\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3223 - accuracy: 0.8835 - val_loss: 0.3446 - val_accuracy: 0.8736 - lr: 0.0082\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.007408182602375746.\n",
      "Epoch 8/10\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3101 - accuracy: 0.8881 - val_loss: 0.3429 - val_accuracy: 0.8790 - lr: 0.0074\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.006703200750052929.\n",
      "Epoch 9/10\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3022 - accuracy: 0.8917 - val_loss: 0.3393 - val_accuracy: 0.8754 - lr: 0.0067\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.006065306719392538.\n",
      "Epoch 10/10\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2934 - accuracy: 0.8940 - val_loss: 0.3352 - val_accuracy: 0.8788 - lr: 0.0061\n"
     ]
    }
   ],
   "source": [
    "models_to_train = [\n",
    "    ('Self-Normalizing NN with Dropout', self_normalizing_nn_with_dropout),\n",
    "    ('NN with Dropout', nn_with_dropout),\n",
    "    ('NN with Batch Normalization', nn_with_batch_normalization)\n",
    "]\n",
    "\n",
    "schedules = [\n",
    "    ('1cycle', lr_schedule_1cycle),\n",
    "    ('Performance', lr_schedule_performance)\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name, model_fn in models_to_train:\n",
    "    for schedule_name, lr_schedule in schedules:\n",
    "        model = model_fn()\n",
    "        model.compile(optimizer=SGD(lr=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        if schedule_name == '1cycle':\n",
    "            lr_callback = LearningRateScheduler(lr_schedule)\n",
    "            history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_val, y_val), callbacks=[lr_callback])\n",
    "        else:\n",
    "            lr_callback = callbacks.LearningRateScheduler(lr_schedule, verbose=1)\n",
    "            history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_val, y_val), callbacks=[lr_callback])\n",
    "        \n",
    "        results[(model_name, schedule_name)] = history.history['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Self-Normalizing NN with Dropout', '1cycle') 0.8358333110809326\n",
      "('Self-Normalizing NN with Dropout', 'Performance') 0.7989166378974915\n",
      "('NN with Dropout', '1cycle') 0.8576666712760925\n",
      "('NN with Dropout', 'Performance') 0.8424999713897705\n",
      "('NN with Batch Normalization', '1cycle') 0.8818333148956299\n",
      "('NN with Batch Normalization', 'Performance') 0.8790000081062317\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for item in results:\n",
    "    print(item,np.max(results[item]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Use hyperparameter tuning with the Fashion MNIST data to explore the following parameters:\n",
    "\n",
    "- Dropout rate\n",
    "- Number of layers (2-4)\n",
    "- Number of neurons per layer (64 - 128)\n",
    "\n",
    "Use Bayesian optimization and tensorboard to examine your results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "callback should be either a callable or a list of callables.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m early_stopping_cb \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m) \n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Perform hyperparameter optimization\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m \u001b[43mbayes_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtensorboard_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43mearly_stopping_cb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Get the best hyperparameters\u001b[39;00m\n\u001b[1;32m     60\u001b[0m best_params \u001b[38;5;241m=\u001b[39m bayes_search\u001b[38;5;241m.\u001b[39mbest_params_\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/skopt/searchcv.py:524\u001b[0m, in \u001b[0;36mBayesSearchCV.fit\u001b[0;34m(self, X, y, groups, callback, **fit_params)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, groups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params):\n\u001b[1;32m    504\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run fit on the estimator with randomly drawn parameters.\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \n\u001b[1;32m    506\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;124;03m        the list is called.\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 524\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callbacks \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    527\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_kwargs_ \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/skopt/callbacks.py:33\u001b[0m, in \u001b[0;36mcheck_callback\u001b[0;34m(callback)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m callback\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     34\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallback should be either a callable or \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma list of callables.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     35\u001b[0m         )\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n",
      "\u001b[0;31mValueError\u001b[0m: callback should be either a callable or a list of callables."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from datetime import datetime\n",
    "\n",
    "# Load and preprocess Fashion MNIST dataset\n",
    "(X_train, y_train), (X_val, y_val) = fashion_mnist.load_data()\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_val = X_val.astype('float32') / 255.0\n",
    "\n",
    "# Define the neural network architecture\n",
    "class MyModel:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y, dropout_rate, num_layers, num_neurons):\n",
    "        model = Sequential()\n",
    "        model.add(Flatten(input_shape=(28, 28)))\n",
    "        for _ in range(num_layers):\n",
    "            model.add(Dense(num_neurons, activation='relu'))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "        model.add(Dense(10, activation='softmax'))\n",
    "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        self.model = model\n",
    "        self.history = self.model.fit(X, y, epochs=3, verbose=0)  # Adjust epochs as needed\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return self.model.evaluate(X, y, verbose=0)[1]\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "search_space = {\n",
    "    'dropout_rate': (0.1, 0.5),\n",
    "    'num_layers': (2, 4),\n",
    "    'num_neurons': (64, 128)\n",
    "}\n",
    "\n",
    "# Set up Bayesian optimization\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=MyModel(),\n",
    "    search_spaces=search_space,\n",
    "    n_iter=20,  # Number of iterations for the search\n",
    "    cv=StratifiedKFold(n_splits=3),\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Set up TensorBoard\n",
    "log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir) \n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=2) \n",
    "\n",
    "# Perform hyperparameter optimization\n",
    "bayes_search.fit(X_train, y_train, callback=[tensorboard_callback,early_stopping_cb])\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = bayes_search.best_params_\n",
    "print(\"Best hyperparameters:\", best_params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
